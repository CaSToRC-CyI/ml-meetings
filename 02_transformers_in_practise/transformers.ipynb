{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa471ae-7bb0-47c0-af2d-2b0d1d700a9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **The Transformer: An implementation in PyTorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d3e53-b38d-4503-b735-3b59bce14c2e",
   "metadata": {},
   "source": [
    "The past years, the field of artificial intelligence has been shaped by the rise of transformer-based models, trained for tasks such as language understanding and generation. Although the original work of Vaswani et al. \"Attention is all you need\" focuses on natural language processing (NLP) tasks, since then many transformer-based models have emerged in different fields, including more modalities i.e. images, video, audio, etc. \n",
    "\n",
    "The most important component of Transformers is the Attention mechanism (last meeting's presentation), which allows the model to pay attention to different parts of the input sentence and assign weights depending on their importance for the predicting word. This mechanism enables Transformers to extract long-range dependencies in data, by considering different parts of the input regardless of their distance from each other.\n",
    "\n",
    "In this work, we will see an implementation of the original and basic Transformer model in PyTorch, tested on a simple setup. The aim of this tutorial is to provide an understanding of how to build such a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b4205-079d-455e-98c5-58fa9e6b90ee",
   "metadata": {},
   "source": [
    "## **Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe3ae39-9824-4f0e-afe2-bf6e64395e67",
   "metadata": {},
   "source": [
    "Transformers are based on an Encoder-Decoder architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52650cf3-625e-4462-83e5-32da47755d42",
   "metadata": {},
   "source": [
    "<img src=\"images/trans.png\" width=\"450\" height=\"600\" style=\"margin:auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4638c61-83f1-4ce5-8681-77f6fa3d7137",
   "metadata": {},
   "source": [
    "There are five core processes that we need to understand to implement this model:\n",
    "\n",
    "* **Embedding of the inputs**\n",
    "* **Positional Encoding**\n",
    "* **Masks**\n",
    "* **Multi-Head Attention layer**\n",
    "* **Feed-Forward layer**\n",
    "\n",
    "For the rest, we assume that we are in the context of next word prediction in a machine translation task. In this context, each word is split into tokens so the model performs next token prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d59b1-baa4-46be-88fc-cc744706a7a2",
   "metadata": {},
   "source": [
    "## **Importing the necessary libraries and modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fd1609-302e-4423-a62a-060721279f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvme/h/gkourmouli/.conda/envs/multimae/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6fe54f-b5b0-4ecb-b56d-55e73500e1ae",
   "metadata": {},
   "source": [
    "## **Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f7b65-c1bf-41ee-bc4e-ebcc24a228e1",
   "metadata": {},
   "source": [
    "Embeddings are low-dimensional, dense vector representations of high-dimensional data. They are used to capture the underlying structure and relationships within the data in a more compact and meaningful way. They are common in NLP, but they can be applied to various types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52710e1-b39f-478e-befb-3d1828622797",
   "metadata": {},
   "source": [
    "Embeddings are handled simply in pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb5539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = nn.Embedding(vocab_size, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74810355-056f-41e2-8db8-8dcc4e69871f",
   "metadata": {},
   "source": [
    "The above code assign every input word/token to an embedding vector, which in the training phase will be learnt by the model as trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327ef9e-4dce-464e-8bab-d965d6652ea5",
   "metadata": {},
   "source": [
    "## **Positional Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5ec96-d7de-41c7-bf9f-035845481eb3",
   "metadata": {},
   "source": [
    "In order for the model to perform the next token prediction task, it needs to understand and find patterns within a sentence. For the model, this means understanding the meaning of each token and its position in the sentence.\n",
    "\n",
    "The meaning of the tokens is learned by the embedding vector. What about their position in the sentence?\n",
    "\n",
    "The positions are learned by the positional encodings. In the architecture:\n",
    "\n",
    "<img src=\"images/position.png\" style=\"margin:auto\"/>\n",
    "\n",
    "They are functions that create values for each position through the formulas:\n",
    "\n",
    "<img src=\"images/positional.png\" style=\"margin:auto\"/>\n",
    "\n",
    "where *pos* represents the order/position of a token in the sentence and *i* its dimension. \n",
    "\n",
    "The sinusoidal functions are chosen to allow the model to learn to attend to relative positions through a more unique and smooth encoding for each position than the assignment of a simple natural number i.e. 0, 1, ...\n",
    "\n",
    "The above equations then create a constant 2D matrix that stores all the information about the positions of the tokens. Eventually, this matrix will be added to the input/source matrix which has all the input sentences. After the addition, the input sequence will gain some information about the position of the tokens and each token embedding will be altered based on its corresponding position vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed05b54-e50d-49d7-8a88-cd904c772092",
   "metadata": {},
   "source": [
    "In PyTorch this is translated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf8e4c03-da69-4c7d-8b7d-ebce738fedb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\" Computation of positional encodings\n",
    "        based on sin and cos functions\n",
    "    \"\"\"\n",
    "    def __init__(self, max_length, d_model, device):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "\n",
    "        self.pos_encod = torch.zeros(max_length, d_model, device=device)\n",
    "\n",
    "        # create indices pos = [0, 1,..., max_length] and 2i = [0, 2,..., d_model]\n",
    "        pos = torch.arange(0, max_length, device=device).float().unsqueeze(dim=1)\n",
    "        i2 = torch.arange(0, d_model, step=2, device=device).float()\n",
    "\n",
    "        # compute sinusoids\n",
    "        # sin: for even indices\n",
    "        # cos: for odd indices\n",
    "        self.pos_encod[:, 0::2] = torch.sin(pos / (10000**(i2/d_model)))\n",
    "        self.pos_encod[:, 1::2] = torch.cos(pos / (10000**(i2/d_model)))\n",
    "                                                \n",
    "    def forward(self, x):\n",
    "        N, seq_size = x.shape\n",
    "\n",
    "        pos_embedding = self.pos_encod[:seq_size, :]\n",
    "        pos_embedding = pos_embedding.expand(N, seq_size, self.d_model)\n",
    "        pos_embedding = pos_embedding.to(self.device)\n",
    "        # print('positional encoding shape', pos_embedding.shape, '\\n')\n",
    "\n",
    "        return pos_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3578df-5fc2-4664-8396-98f3bc66571f",
   "metadata": {},
   "source": [
    "Variables in PositionalEncoding class:\n",
    "* max_length: Maximum length of all sequences - sentences longer than this will be filtered out\n",
    "* d_model: Inner dimension of the model's input - the dimension of the embedding vector\n",
    "* device: The device on which the calculations will be performed - cpu or gpu\n",
    "* pos_encod: Tensor that will store all the positional encodings\n",
    "* pos: Tensor with all the positions in the sentence\n",
    "* i2: Scaling term for position indices\n",
    "\n",
    "And some basic variables that we will use throughout the notebook:\n",
    "\n",
    "* N, seq_size: # of inputs, size of each input\n",
    "* x: input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66829edd-325f-4b8e-ae99-67d94ece0ea8",
   "metadata": {},
   "source": [
    "## **Masks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147481ca-107a-4e14-aea9-77738bde88af",
   "metadata": {},
   "source": [
    "In Transformers we usually use two types of masks:\n",
    "\n",
    "* **Padding Mask**: It is used in the encoder and decoder and basically hides the paddings that were added to the source and target sequences, so that the model will not make any computations for them. \n",
    "\n",
    "* **No-peak Mask**: It is used in the decoder to prevent it from seeing the next tokens of the target sequence when predicting the next word. With this mask decoder is able to see only up to the word it has predicted in the previous round."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794f8d7-bfe2-4794-b1c1-3c65bf2c800c",
   "metadata": {},
   "source": [
    "The mask for the source sequence - Padding Mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2c87046-3827-484b-b9af-ee6182f4f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_src_mask(self, src):\n",
    "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    # resulting mask shape: (N, 1, 1, src_length)\n",
    "    src_mask = src_mask.to(self.device)\n",
    "    return src_mask     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c6cbb-4566-42e2-a682-a87cb441e091",
   "metadata": {},
   "source": [
    "The mask for the target sequence - Padding & No-Peak Mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd849373-e40f-4e17-8fe5-de8ffd0078d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trg_mask(self, trg):\n",
    "    N, trg_length = trg.shape\n",
    "    trg_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    trg_mask = trg_mask.to(self.device)\n",
    "\n",
    "    # create a lower triangular matrix so that decoder cannot see following words\n",
    "    no_peak_mask = torch.tril(torch.ones((trg_length, trg_length)))\\\n",
    "               .expand(N, 1, trg_length, trg_length)\n",
    "    no_peak_mask = Variable(torch.tensor(no_peak_mask == 1).to(self.device))\n",
    "    no_peak_mask = no_peak_mask.to(self.device)\n",
    "\n",
    "    trg_mask = trg_mask & no_peak_mask\n",
    "    return trg_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712088b5-4a86-4bc6-a138-175205bf7a27",
   "metadata": {},
   "source": [
    "Variables in mask functions:\n",
    "* src, trg: Source and target sequences\n",
    "* src_mask, trg_mask: Mask for the input/source and target sequence\n",
    "* src_pad_idx, trg_pad_idx: Padding indices for source and target sequence\n",
    "* src_length, trg_length: Length of source and target sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36072f-a819-45c2-adc1-056947f80db6",
   "metadata": {},
   "source": [
    "The input of the encoder is the source sentence and that of the decoder is the translated sentence. After the first Multi-Head Attention calculations, the decoder takes also all the outputs of the encoder and the translated sentence up to the word which it is predicting and makes its predictions. By using the no-peak mask, we prevent the decoder from calculating attention scores based on the following words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ceebd7-c51b-4c34-bda8-4272ab984cf6",
   "metadata": {},
   "source": [
    "## **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fbb10-8882-43dd-b08b-c413701eeaf7",
   "metadata": {},
   "source": [
    "The Multi-Head attention mechanism computes the attention between each pair of words in a sequence. It consists of multiple attention heads and each of them attends to a different part of the input sequence. \n",
    "\n",
    "The attention layer has two components:\n",
    "\n",
    "* **Self Attention** or **Scaled Dot-Product Attention**\n",
    "* **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711faef-f1a4-42fc-8661-172fed7abce1",
   "metadata": {},
   "source": [
    "### **Self Attention**\n",
    "\n",
    "To calculate the attention we use the following equation:\n",
    "\n",
    "<img src=\"images/attention.png\" style=\"margin:auto\"/>\n",
    "\n",
    "The steps to calculate it:\n",
    "\n",
    "<img src=\"images/selfattention.png\" style=\"margin:auto\"/>\n",
    "\n",
    "where V, Q, K are the values, queries, and keys respectively. \n",
    "\n",
    "Initially, we must multiply Q with the traspose of K and the result is then scaled by the square root of *d_k = d_model/# of heads*. Before calculating the Softmax we can optionally apply a mask so that the padded indices are filtered out and in the case of decoder the no-peak mask. We then perform a dot-product between the result and V. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1bb0ab1-3686-41a7-a4f8-f490c349bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask=None):\n",
    "        # compute matrix multiplication and scale it\n",
    "        # keys shape: (N, key_len, n_heads, d_head)\n",
    "        # queries shape: (N, query_len, n_heads, d_head)\n",
    "        # product shape: (N, n_heads, query_len, key_len) \n",
    "        # scale factor: sqrt(d_head)\n",
    "        product = torch.einsum('nqhd,nkhd->nhqk', [queries, keys])\n",
    "\n",
    "        # apply mask (optional)\n",
    "        if mask is not None:\n",
    "            product = product.masked_fill(mask==0, float('-1e20'))\n",
    "\n",
    "        # compute attention and pass it from softmax\n",
    "        scaled_product = product/math.sqrt(keys.shape[3])\n",
    "        attention = self.softmax(scaled_product)\n",
    "\n",
    "        # compute attention\n",
    "        # attention shape: (N, n_heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, n_heads, d_head)\n",
    "        # always value_len=key_len so we multiply across that dimension\n",
    "        # out shape: (N, query_len, n_heads, d_head)\n",
    "        out = torch.einsum('nhqk,nkhd->nqhd', [attention, values])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966536a-74a1-4266-8959-cb171fe4627d",
   "metadata": {},
   "source": [
    "Variables in ScaledDotProductAttention class:\n",
    "* n_heads: # of heads\n",
    "* d_head: Dimension of each head and equal to *d_model/n_heads*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc84f3-b6e3-4063-b598-c6118c118c77",
   "metadata": {},
   "source": [
    "### **Multi-Head Attention**\n",
    "\n",
    "In MHA each input is split into multiple heads so that the model can attend *simultaneously* to different parts of the embedding. We split the embedding vector into N heads and each of them receives an input of dimension *N * seq_length * (d_model/# of heads)*.\n",
    "\n",
    "<img src=\"images/multihead.png\" style=\"margin:auto\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82e1cc82-5723-4829-945c-d9d24c1bd1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model          # embedding size\n",
    "        self.n_heads = n_heads          # h in paper\n",
    "        self.d_head = d_model//n_heads  # integer division, d_k,d_v in paper\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "        # assertion to ensure that d_model is divided by n_heads\n",
    "        assert (self.d_head*n_heads == d_model), 'The number of heads should\\\n",
    "                divide the dimension of the model'\n",
    "\n",
    "        # Linear layers to pass V,K,Q + final linear layer (after concat)\n",
    "        self.values_lin = nn.Linear(self.d_head, self.d_head, bias=False)\n",
    "        self.keys_lin = nn.Linear(self.d_head, self.d_head, bias=False)\n",
    "        self.queries_lin = nn.Linear(self.d_head, self.d_head, bias=False)\n",
    "        self.fc_out = nn.Linear(self.d_head*n_heads, self.d_model)      # concatenation of heads outputs\n",
    "\n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        N = queries.shape[0]   # # of training samples that we send in the model at the same time\n",
    "        value_len = values.shape[1]\n",
    "        key_len = keys.shape[1]\n",
    "        query_len = queries.shape[1]\n",
    "\n",
    "        # split the inputs V,K,Q into head parts and create matrices\n",
    "        values = values.reshape(N, value_len, self.n_heads, self.d_head)\n",
    "        keys = keys.reshape(N, key_len, self.n_heads, self.d_head)\n",
    "        queries = queries.reshape(N, query_len, self.n_heads, self.d_head)\n",
    "\n",
    "        # pass them from corresponding linear layers\n",
    "        values = self.values_lin(values)       \n",
    "        keys = self.keys_lin(keys)       \n",
    "        queries = self.queries_lin(queries)\n",
    "\n",
    "        # compute scale dot-product attention\n",
    "        attention = self.attention(values, keys, queries, mask)\n",
    "\n",
    "        # concatenate the results from scale dot-product attention \n",
    "        # attention shape: (N, query_len, n_heads, d_head)\n",
    "        # so we flatten the last two dimensions n_heads*d_head = d_model\n",
    "        # concatenated shape: (N, query_len, d_model)\n",
    "        concatenated = attention.reshape(N, query_len, self.n_heads*self.d_head)\n",
    "\n",
    "        # pass the concat tensor from linear layer\n",
    "        # output shape: (N, query_len, d_model)\n",
    "        out = self.fc_out(concatenated)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63e05c-73f5-44b2-b894-e9aee0cb501f",
   "metadata": {},
   "source": [
    "Variables in MultiHeadAttention class:\n",
    "* d_model: Embedding size\n",
    "* n_heads: # of heads\n",
    "* d_head: Dimension of each head and equal to *d_model/n_heads*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0bdcb-1931-43cf-b9df-2f316694e219",
   "metadata": {},
   "source": [
    "## **Feed-Forward Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296b13c-4e30-4862-8f82-13a616ed8602",
   "metadata": {},
   "source": [
    "The position-wise feed-forward neural network consists of two linear layers with a ReLU activation function in between. It is applied to each position separately, and it processes and transforms the attention outputs. These type of networks makes the models deeper by using linear functions and better at analysing patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b92a41c-7222-41e4-8019-31589de7d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.feed_forward = nn.Sequential(self.linear1, \n",
    "                                        self.relu, \n",
    "                                        self.linear2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.feed_forward(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2595d-a750-4570-ad49-e67dfc690785",
   "metadata": {},
   "source": [
    "Variables in PositionWiseFeedForward class:\n",
    "* d_model: Embedding size\n",
    "* d_ff: Inner dimension of feed forward operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb062845-1d85-4500-853d-6adc6ed95f61",
   "metadata": {},
   "source": [
    "## **Transformer Block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23505afc-15cd-4d92-b17a-fabe647047c4",
   "metadata": {},
   "source": [
    "In both encoder and decoder there is a Transformer block:\n",
    "\n",
    "<img src=\"images/transfblock.png\" style=\"margin:auto\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505a9d0-0288-452c-b748-bea1161c47ac",
   "metadata": {},
   "source": [
    "It consists of:\n",
    "* **MHA**\n",
    "* **Feed-Forward Network**\n",
    "* **Normalization**\n",
    "\n",
    "Normalization prevents the range of values from changing too much by keeping all the values within a certain range. This way the model trains faster and has better ability to generalise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b184774d-b85c-442c-96db-d70fc2917ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\" One transformer block:\n",
    "\n",
    "        Multi-head attention \n",
    "        Skip connection and normalization layer\n",
    "        Feed forward layer\n",
    "        Skip connection and normalization layer\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout, d_ff):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "\n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        attention = self.attention(values, keys, queries, mask)\n",
    "        x = self.dropout(self.norm1(attention + queries))  # skip connection 'attention + queries'\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))   # skip connection 'forward + x'\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d846a-278e-485a-b9e4-cb19a3c70716",
   "metadata": {},
   "source": [
    "Variables in TransformerBlock class:\n",
    "* d_model: Embedding size\n",
    "* n_heads: # of heads\n",
    "* dropout: Dropout operation\n",
    "* d_ff: Inner dimension of feed forward operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562eb912-f998-416f-98dd-85d8fcf2953d",
   "metadata": {},
   "source": [
    "## **Encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84914fe-ccda-4eec-9cfa-0bf047d5bbc5",
   "metadata": {},
   "source": [
    "We will now build the Encoder layer. The Transformer model repeats this layer *Nx* times to produce the values and keys from the source sentence which will then be fed to the decoder.\n",
    "\n",
    "<img src=\"images/encoder.png\" style=\"margin:auto\"/>\n",
    "\n",
    "It consists of:\n",
    "* **Embeddings of input**\n",
    "* **Positional Encodings**\n",
    "* **Transformer Block**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a34b0ced-4593-4db8-94ef-7392842db2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" One encoder block ≡ one transformer block\n",
    "        One encoder block with the computations of its input layer:\n",
    "\n",
    "        Tranform input into an embedding\n",
    "        Compute positional encoding of the input sequence\n",
    "        Create the input for the transformer (addition of the two above)\n",
    "        Transformer block\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab_size, d_model, n_layers, n_heads, \n",
    "                dropout, d_ff, max_length, device):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.position_embedding = PositionalEncoding(max_length, d_model, device)\n",
    "\n",
    "        # version 2 of positional encoding\n",
    "        # self.position_embedding = nn.Embedding(max_length, d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(d_model, n_heads, dropout, d_ff)\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # version 2 of positional encoding for each of the input sequences        \n",
    "        # N, seq_size = x.shape\n",
    "        # positions = torch.arange(0, seq_size)\n",
    "        # positions = positions.expand(N, seq_size).to(self.device)\n",
    "        # embedding = self.word_embedding(x) + self.position_embedding(positions)\n",
    "        # print('encoder positional encoding shape', self.position_embedding(positions).shape)\n",
    "\n",
    "        # input embedding with the positional encodings\n",
    "        # print('encoder')\n",
    "        embedding = self.word_embedding(x) + self.position_embedding(x)\n",
    "        out = self.dropout(embedding)\n",
    "\n",
    "        # layers\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e57fc0b-a442-4e1e-985b-7aade89a6a84",
   "metadata": {},
   "source": [
    "Variables in Encoder class:\n",
    "* src_vocab_size: The size of the source vocabulary\n",
    "* d_model: Embedding size\n",
    "* n_layers: # of layers\n",
    "* n_heads: # of heads\n",
    "* dropout: Dropout operation\n",
    "* d_ff: Inner dimension of feed forward operations\n",
    "* max_length: Maximum sequence length\n",
    "* device: CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03b8ad0-5c49-4c2f-83f1-18b853dfa0d9",
   "metadata": {},
   "source": [
    "## **Decoder Block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc51fe06-d160-4c44-942c-01e3a723baf0",
   "metadata": {},
   "source": [
    "To build the decoder we first create the decoder block. The Transformer model repeats this layer *Nx* times to predict the next word.\n",
    "\n",
    "<img src=\"images/decoderblock.png\" style=\"margin:auto\"/>\n",
    "\n",
    "It consists of:\n",
    "* **Masked MHA**: So that the decoder cannot see *future* predictions\n",
    "* **Transformer Block**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd92f0f9-9ecf-4a9a-bcc5-829d876b9293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\" One decoder block ≡ multi-head attention + \n",
    "                            skip connection and normalization layer + \n",
    "                            one transformer block\n",
    "                            \n",
    "        One decoder block with the computations of its input layer:\n",
    "\n",
    "        Tranform input into an embedding\n",
    "        Compute positional encoding of the input sequence\n",
    "        Create the input for the first layer (addition of the two above)\n",
    "        Multi-head attention \n",
    "        Skip connection and normalization layer\n",
    "        Transformer block (keys, values from encoder + queries from the input)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout, d_ff):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.transformer_block = TransformerBlock(d_model, n_heads, dropout, d_ff)\n",
    "\n",
    "    # values and keys come from the encoder\n",
    "    # x is the input of the decoder\n",
    "    def forward(self, values, keys, x, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)  # masked MHA in decoder so we need to pass target mask\n",
    "        queries = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(values, keys, queries, src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c948f-ce2a-4a2c-88f8-b56a6e579047",
   "metadata": {},
   "source": [
    "Variables in DecoderBlock class:\n",
    "* d_model: Embedding size\n",
    "* n_heads: # of heads\n",
    "* dropout: Dropout operation\n",
    "* d_ff: Inner dimension of feed forward operations\n",
    "* x: Input of decoder - target sequence\n",
    "* src_mask, trg_mask: Source and target mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a8f380-8d12-4519-996e-5b68fff6b257",
   "metadata": {},
   "source": [
    "## **Decoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4703e-3afc-40f4-b4cc-2aec9fdb8016",
   "metadata": {},
   "source": [
    "We will now build the Decoder layer using the Decoder Block that we created previously:\n",
    "\n",
    "<img src=\"images/decoder.png\" style=\"margin:auto\"/>\n",
    "\n",
    "It consists of:\n",
    "* **Embeddings of decoder input**: This is the target sequence\n",
    "* **Positional Encodings**\n",
    "* **Decoder Block**\n",
    "* **Linear Projection**: To project decoder's output to desired dimensions\n",
    "* **Softmax**: To produce probabilities from predictions (logits)\n",
    "\n",
    "The above combination enables the decoder to generate meaningful outputs based on the encoder's source representations, taking into account both the target sequence and the source sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d417d592-6590-4e7d-8cf0-6ca587a672be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, trg_vocab_size, d_model, n_layers, n_heads, \n",
    "                dropout, d_ff, max_length, device):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, d_model)\n",
    "        self.position_embedding = PositionalEncoding(max_length, d_model, device)\n",
    "\n",
    "        # version 2 of positional encoding\n",
    "        # self.position_embedding = nn.Embedding(max_length, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(d_model, n_heads, dropout, d_ff)\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # CHANGE if in training phase we want softmax on the output \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, encoder_out, x, src_mask, trg_mask):\n",
    "        # version 2 of positional encoding for each of the input sequences        \n",
    "        # N, seq_size = x.shape\n",
    "        # positions = torch.arange(0, seq_size)\n",
    "        # positions = positions.expand(N, seq_size).to(self.device)\n",
    "        # embedding = self.word_embedding(x) + self.position_embedding(positions)\n",
    "        # print('decoder positional encoding shape', self.position_embedding(positions).shape)\n",
    "\n",
    "        # input embedding with the positional encodings\n",
    "        # print('decoder')\n",
    "        embedding = self.word_embedding(x) + self.position_embedding(x)\n",
    "        x = self.dropout(embedding)\n",
    "\n",
    "        # layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(encoder_out, encoder_out, x, src_mask, trg_mask)\n",
    "\n",
    "        # last linear layer and softmax\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        # CHANGE if in training phase we want softmax on the output\n",
    "        out = self.softmax(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e19c9d-9bff-4174-87f3-3962e4933abf",
   "metadata": {},
   "source": [
    "Variables in Decoder class:\n",
    "* trg_vocab_size: The size of the target vocabulary\n",
    "* n_layers: # of layers\n",
    "* d_model: Embedding size\n",
    "* n_heads: # of heads\n",
    "* dropout: Dropout operation\n",
    "* d_ff: Inner dimension of feed forward operations\n",
    "* max_length: Maximum sequence length\n",
    "* device: CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93fb194-a688-4b32-aee6-7e40171ef17c",
   "metadata": {},
   "source": [
    "## **Putting it all together**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbabff7-c69f-4b92-bbe1-58234ce80f5a",
   "metadata": {},
   "source": [
    "We will now combine the Encoder and Decoder layers to create the Transformer model:\n",
    "\n",
    "<img src=\"images/trans.png\" style=\"margin:auto\"/>\n",
    "\n",
    "During initialization, we can define all the hyperparameters of the model (e.g. n_layers, n_heads, etc.) or we can keep the default values stated in the Transformer class which in this case are the values from the original paper. So the Transformer model is able to process input sequences, extract patterns, and produce output sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7644a9e5-059b-45bc-b7ad-10aeb8878272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\" Putting everything together to form Transformer\n",
    "\n",
    "        Variables:\n",
    "        src_pad_idx, trg_pad_idx: necessary to compute source and target masks\n",
    "\n",
    "        Functions\n",
    "        make_src_mask:  create masks for source\n",
    "                        if it is a source pad index then set to 0\n",
    "                        if it is not set to 1 \n",
    "        make_trg_mask: create masks for target\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,\n",
    "                 d_model=512, n_layers=6, n_heads=8, device=\"cuda\", d_ff=2048, \n",
    "                 dropout=0, max_length=100):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, n_layers, n_heads, \n",
    "                                dropout, d_ff, max_length, device)\n",
    "        self.decoder = Decoder(trg_vocab_size, d_model, n_layers, n_heads, \n",
    "                                dropout, d_ff, max_length, device)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # resulting mask shape: (N, 1, 1, src_length)\n",
    "        src_mask = src_mask.to(self.device)\n",
    "        return src_mask     \n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_length = trg.shape\n",
    "        trg_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_mask = trg_mask.to(self.device)\n",
    "        \n",
    "        # create a lower triangular matrix so that decoder cannot see following words\n",
    "        no_peak_mask = torch.tril(torch.ones((trg_length, trg_length)))\\\n",
    "                   .expand(N, 1, trg_length, trg_length)\n",
    "        no_peak_mask = Variable(torch.tensor(no_peak_mask == 1).to(self.device))\n",
    "        no_peak_mask = no_peak_mask.to(self.device)\n",
    "    \n",
    "        trg_mask = trg_mask & no_peak_mask\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        encoder_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(encoder_src, trg, src_mask, trg_mask)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83923224-fb61-48e7-a181-ce92d4f23a18",
   "metadata": {},
   "source": [
    "## <h1><center>$\\cdots$</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8dc3d3-cf95-4f2f-b356-6d8d865d321c",
   "metadata": {},
   "source": [
    "## **Simple example**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce4691f-e5fc-4752-a2c4-aa5899259eca",
   "metadata": {},
   "source": [
    "In this very simple example, we will create a toy dataset just to check that the code is working and the model can produce outputs. In practice, such a model needs a larger dataset, some preprocessing steps, tokenization, vocabulary mappings, and training for a downstream task.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02d040c6-c103-4d01-a585-ab42aef5c949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source shape: torch.Size([2, 10]) \n",
      "target shape: torch.Size([2, 9]) \n",
      "target shifted right shape: torch.Size([2, 8]) \n",
      "output shape: torch.Size([2, 8, 10]) \n",
      "\n",
      "predictions size\n",
      " torch.Size([2, 10])\n",
      "predictions\n",
      " tensor([[1, 2, 7, 6, 3, 4, 5, 5, 1, 3],\n",
      "        [2, 4, 2, 5, 1, 2, 0, 0, 3, 7]], device='cuda:0') \n",
      "\n",
      "model Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (word_embedding): Embedding(10, 512)\n",
      "    (position_embedding): PositionalEncoding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionWiseFeedForward(\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (relu): ReLU()\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionWiseFeedForward(\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (relu): ReLU()\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionWiseFeedForward(\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (relu): ReLU()\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionWiseFeedForward(\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (relu): ReLU()\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionWiseFeedForward(\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (relu): ReLU()\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionWiseFeedForward(\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (relu): ReLU()\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (word_embedding): Embedding(10, 512)\n",
      "    (position_embedding): PositionalEncoding()\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): PositionWiseFeedForward(\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (relu): ReLU()\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (feed_forward): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): ReLU()\n",
      "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): PositionWiseFeedForward(\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (relu): ReLU()\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (feed_forward): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): ReLU()\n",
      "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): PositionWiseFeedForward(\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (relu): ReLU()\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (feed_forward): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): ReLU()\n",
      "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): PositionWiseFeedForward(\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (relu): ReLU()\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (feed_forward): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): ReLU()\n",
      "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): PositionWiseFeedForward(\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (relu): ReLU()\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (feed_forward): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): ReLU()\n",
      "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (values_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (keys_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (queries_lin): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): PositionWiseFeedForward(\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (relu): ReLU()\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (feed_forward): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): ReLU()\n",
      "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_170406/698584254.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  no_peak_mask = Variable(torch.tensor(no_peak_mask == 1).to(self.device))\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    n_layers = 6\n",
    "    d_model = 512\n",
    "    d_ff = 2048\n",
    "    n_heads = 8\n",
    "    dropout = 0.1\n",
    "    max_length = 20\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    src_pad_idx = 0\n",
    "    trg_pad_idx = 0\n",
    "    src_vocab_size = 10\n",
    "    trg_vocab_size = 10\n",
    "    \n",
    "    src = torch.tensor([[2,3,2,6,8,4,9,5,1,0], [3,5,7,3,7,9,2,7,8,1]]) \n",
    "    # src = torch.randint(1, src_vocab_size, (3, max_length)) # size (batch_size, seq_length)\n",
    "    src = src.to(device)\n",
    "    trg = torch.tensor([[3,5,7,8,9,2,1,0,0], [2,4,5,8,3,1,0,0,0]])\n",
    "    # trg = torch.randint(1, trg_vocab_size, (3, max_length))  # size (batch_size, seq_length)\n",
    "    trg = trg.to(device)\n",
    "\n",
    "    model = Transformer(\n",
    "            src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,\n",
    "            d_model, n_layers, n_heads, device, d_ff, dropout, max_length\n",
    "            )\n",
    "    model = model.to(device)\n",
    "\n",
    "    # send in model src as is and trg without the last token that we want to predict\n",
    "    out = model(src, trg[:, :-1])\n",
    "    _, preds = torch.max(out, dim=1)\n",
    "\n",
    "    print('source shape:', src.shape, \n",
    "          '\\ntarget shape:', trg.shape,\n",
    "          '\\ntarget shifted right shape:', trg[:, :-1].shape,\n",
    "          '\\noutput shape:', out.shape, '\\n')\n",
    "    print('predictions size\\n', preds.shape)\n",
    "    print('predictions\\n', preds, '\\n')\n",
    "    \n",
    "    # print(trg)\n",
    "    # print(out)\n",
    "#     print(_)\n",
    "    print('model', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2911d74-53b4-49bf-ab89-bb6f2dd3e5c3",
   "metadata": {},
   "source": [
    "## <h1><center>$\\cdots$</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d684d038-946f-4015-a141-5b7a9d9c117f",
   "metadata": {},
   "source": [
    "## **References**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f3102-0f03-4832-9de2-b0581184a5e9",
   "metadata": {},
   "source": [
    "### **Attention is all you need**\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimae-kernel",
   "language": "python",
   "name": "multimae-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
